# Epstein-File-Explorer — Cursor AI Rules

## Project identity

- **Name:** Epstein File Explorer
- **Purpose:** Public record explorer for the DOJ Epstein Files (3.5M+ pages across 12 data sets)
- **Repo:** reconsumeralization/Epstein-File-Explorer (fork of Donnadieu/Epstein-File-Explorer)
- **License:** MIT | **Live:** epstein-file-explorer.com

## Stack

- **Runtime:** Node.js 20+, ESM (`"type": "module"`)
- **Language:** TypeScript 5.6, strict mode
- **Frontend:** React 18, Vite 7, Tailwind 3, shadcn/ui, Radix, D3 (force graphs), Recharts, Framer Motion, Wouter, TanStack React Query
- **Backend:** Express 5, Drizzle ORM, Zod, WebSocket (ws)
- **Database:** PostgreSQL, full-text search (pg_trgm), schema in `shared/schema.ts`
- **Storage:** Cloudflare R2 via @aws-sdk/client-s3
- **AI:** DeepSeek (OpenAI-compat), OpenRouter for chat
- **Pipeline:** aria2c, pdfjs-dist, p-limit, Cheerio | **Deploy:** Fly.io, Docker multi-stage

## Directory structure

- `client/src/` — React frontend (pages, components, hooks, lib)
- `server/` — Express API (routes.ts, storage.ts, db.ts, r2.ts, chat/)
- `shared/schema.ts` — Single source of truth: Drizzle schema + Zod types
- `scripts/pipeline/` — 14-stage ETL (TypeScript, `npx tsx`)
- `data/downloads/` — Torrent downloads (gitignored, 300GB+)
- `data/extracted/` — PDF text extraction output (gitignored)
- `data/ai-analyzed/` — DeepSeek JSON (tracked)
- `data/ai-analyzed/invalid/` — Quarantined invalid LLM output (gitignored)

## Pipeline architecture (scripts/pipeline/)

**Order:** scrape-wikipedia → download-torrent → upload-r2 → process → ds9-gap-analysis → classify-media → analyze-ai → load-persons → load-documents → import-downloads → load-ai-results → extract-connections → update-counts → dedup-persons → dedup-connections

**Key files:**

- `run-pipeline.ts` — Orchestrator + CLI (PipelineConfig, stage dispatch)
- `torrent-downloader.ts` — BitTorrent via aria2c; **safeResolve/safeRmSync** for path safety
- `ai-analyzer.ts` — Regex + DeepSeek, Zod validation, dry-run, quarantine for invalid output
- `pdf-processor.ts` — pdfjs-dist; --max-file-size-mb, --max-concurrent-pdfs, pLimit
- `db-loader.ts` — DB loading with batched transactions (e.g. 100 rows/tx for persons)
- `ds9-gap-analysis.ts` — DS9 EFTA ID gaps → recovery manifest (standalone or pipeline; no DB)
- `doj-scraper.ts`, `wikipedia-scraper.ts` — DOJ + Wikipedia scrapers

## Database (shared/schema.ts)

**Tables:** persons, documents, connections, events, documentPersons, pages, savedSearches, searchHistory, bookmarks  
**Notable columns:** persons (name, aliases, role, description, category, documentCount, connectionCount, profileSections, wikipediaUrl, topContacts); documents (title, dataSet, sourceUrl, documentType, aiAnalysisStatus, r2Key, eftaNumber, aiCostCents); connections (fromPersonId, toPersonId, connectionType, documentIds); events (title, date, personIds, documentIds).

## Coding conventions — enforce

1. **ESM only** — import/export; `import.meta.url` for __dirname.
2. **Drizzle ORM** for all DB — no raw SQL in app code (setup-fts.sql excepted).
3. **Zod** at every trust boundary: API input, LLM output, parsed torrent/scraper data.
4. **p-limit** for concurrency — no unbounded Promise.all on I/O.
5. **Pipeline stages idempotent** — safe to re-run.
6. **Path safety** — `safeResolve(baseDir, candidate)` for paths from external input (torrent names, user input, scrapers).
7. **Batched transactions** — bulk DB via `db.transaction()` with bounded batch size (e.g. 100).
8. **No `any` at trust boundaries** — LLM/API/JSON go through Zod before use.
9. **Structured logging** — stage name, counts, timing; errors with stack.
10. **TypeScript strict** — `tsc --noEmit` must pass before commit.

## Security and data safety — always apply

- **Never** commit .env, API keys, DB credentials, or secrets.
- **Never** trust torrent filenames without path traversal checks.
- **Never** trust LLM output without schema validation.
- **Never** expose victim names or PII — flag redaction failures for <efta@usdoj.gov>; do not publish recovered text.
- All data is from publicly released government records only.
- Treat pipeline input as potentially adversarial.

## Environment

- **Required:** DATABASE_URL
- **AI:** DEEPSEEK_API_KEY
- **R2:** R2_ACCOUNT_ID, R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_BUCKET_NAME
- **Chat UI:** AI_INTEGRATIONS_OPENROUTER_API_KEY, AI_INTEGRATIONS_OPENROUTER_BASE_URL
- **Optional:** PORT (3000), NODE_ENV

## Hardening (fork) — done

Path safety (torrent-downloader), Zod + quarantine (ai-analyzer), PDF guardrails (pdf-processor), AI dry-run, batched DB (db-loader), DS9 gap analysis stage, worktree setup (.env.example, setup.bat/sh, Cursor configs).

## Planned next

DOJ recovery scraper (DS9 gaps), master index CSV/JSON export, redaction-audit stage, document dedup (SimHash/MinHash), archive verify (SHA256 vs community manifest), re-OCR for low-quality text, DOJ diff tracking, DS9 email thread reconstruction.

## CLI (run-pipeline)

```bash
npx tsx scripts/pipeline/run-pipeline.ts [stages] [options]
# Stages: all | quick | full-discovery | analyze-priority | ds9-gap-analysis | or individual stage names
# Options: --data-sets 1,2,3 --max-downloads N --max-process N --rate-limit N --types pdf,jpg
#   --budget N --priority 1-5 --batch-size N --concurrency N --dry-run
#   --max-file-size-mb N --max-concurrent-pdfs N --expected-max-id N (DS9)
```

## What NOT to do

- Don’t use raw SQL for app queries; use Drizzle.
- Don’t skip Zod on API/LLM/parsed JSON.
- Don’t run unbounded parallel I/O; use p-limit.
- Don’t build paths from user/torrent input without safeResolve.
- Don’t commit secrets or expose victim information.
